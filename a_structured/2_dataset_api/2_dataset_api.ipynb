{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Dataset API\n",
    "\n",
    "**目的**\n",
    "* tf.data を使ってデータを TF モデルを読み込む方法を学ぶ\n",
    "* インメモリに全てのデータをロードする方法と、ディスクからバッチでロードする方法の違いを学ぶ\n",
    "* tf.data を使ってcsv ファイルをディスクからバッチで読み込む方法を学習する\n",
    "\n",
    "このノートブックでは、`tf.data.Dataset` を使用したデータ入力パイプラインの作成方法を学びます<br>\n",
    "データをバッチで読み込めるようにし、 後にそのデータに対して**確率的勾配降下法**を利用できる準備をします。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データをメモリからロードする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットを作成する\n",
    "では、前のラボで生成したデータをまた作成しましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_POINTS = 10\n",
    "# The .constant() method will creates a constant tensor from a tensor-like object.\n",
    "X = tf.constant(range(N_POINTS), dtype=tf.float32)\n",
    "Y = 2 * X + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.Data.Datasetを出力する関数を作成します。この関数は、以下を引数として取得します。\n",
    "\n",
    "- $y= 2x + 10$ という線形関数で合成された $X$ と $Y$ のベクトル\n",
    "- データセットを使用してトレーニングを行う回数 (epochs)\n",
    "- バッチサイズ (batch_size)\n",
    "\n",
    "**注意**: データセットから取得する最後のバッチのデータ数は、指定したバッチサイズと異なる可能性があります。\n",
    "\n",
    "すべてのバッチを同じサイズにしたい場合、以下のように最後のバッチを破棄する必要があります。\n",
    "\n",
    "```python\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(X, Y, epochs, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "    dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "では、この関数を使用して、私たちのデータセットを3つのデータ数のバッチにしながら2回イテレーションを回しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 3\n",
    "EPOCH = 2\n",
    "\n",
    "dataset = create_dataset(X, Y, epochs=EPOCH, batch_size=BATCH_SIZE)\n",
    "\n",
    "for i, (x, y) in enumerate(dataset):\n",
    "    print(\"x:\", x.numpy(), \"y:\", y.numpy())\n",
    "    assert len(x) == BATCH_SIZE\n",
    "    assert len(y) == BATCH_SIZE\n",
    "assert  EPOCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 損失関数と勾配\n",
    "損失や勾配を計算する関数は前のセクションと同じです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_mse(X, Y, w0, w1):\n",
    "    Y_hat = w0 * X + w1\n",
    "    errors = (Y_hat - Y)**2\n",
    "    return tf.reduce_mean(errors)\n",
    "\n",
    "def compute_gradients(X, Y, w0, w1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_mse(X, Y, w0, w1)\n",
    "    return tape.gradient(loss, [w0, w1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トレーニングループ\n",
    "ここでの大きな違いは、トレーニングループの中で、`create_dataset`関数が出力する `tf.data.Dataset` を直接イテレーションすることです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 2\n",
    "LEARNING_RATE = .02\n",
    "\n",
    "MSG = \"STEP {step} - loss: {loss}, w0: {w0}, w1: {w1}\\n\"\n",
    "\n",
    "w0 = tf.Variable(0.0)\n",
    "w1 = tf.Variable(0.0)\n",
    "\n",
    "dataset = create_dataset(X, Y, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "for step, (X_batch, Y_batch) in enumerate(dataset):\n",
    "\n",
    "    dw0, dw1 = compute_gradients(X_batch, Y_batch, w0, w1)\n",
    "    w0.assign_sub(dw0 * LEARNING_RATE)\n",
    "    w1.assign_sub(dw1 * LEARNING_RATE)\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        loss = loss_mse(X_batch, Y_batch, w0, w1)\n",
    "        print(MSG.format(step=step, loss=loss, w0=w0.numpy(), w1=w1.numpy()))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データをディスクからロードする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、先ほど作成したデータセットをロードしていきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは、いくつかのデータを確認してみましょう。\n",
    "カラム名とその順序に注意してください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ../data/taxi-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.data を使用して CSV ファイルを読み込む\n",
    "\n",
    "`tf.data` API は関数を使用して簡単に CSV ファイルを読み込むことができます。\n",
    "\n",
    "[tf.data.experimental.make_csv_dataset](https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset)\n",
    "\n",
    "\n",
    "TFRecord 形式のデータがある場合には、以下を使用します。\n",
    "\n",
    "[tf.data.experimental.make_batched_features_dataset](https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_batched_features_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最初のステップは、\n",
    "\n",
    "- 特徴量列の名前を `CSV_COLUMNS` リストに格納する\n",
    "- デフォルトの値を `DEFAULTS` リストに格納する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMNS = ['fare_amount',\n",
    "            'dayofweek',\n",
    "            'hourofday',\n",
    "            'pickup_longitude',\n",
    "            'pickup_latitude',\n",
    "            'dropoff_longitude',\n",
    "            'dropoff_latitude',\n",
    "           ]\n",
    "LABEL_COLUMN = 'fare_amount'\n",
    "DEFAULTS = [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、`make_csv_dataset` を独自の関数でラッピングし、glob などのファイルパターンのパスからデータを取得できるようにしましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(pattern):\n",
    "    return tf.data.experimental.make_csv_dataset(\n",
    "        pattern, 1, CSV_COLUMNS, DEFAULTS)\n",
    "\n",
    "tempds = create_dataset('../data/taxi-train*')\n",
    "print(tempds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このデータがプリフェッチされたデータセットであることに注意してください。このデータは、各要素をキーに特徴量名をとり、値に`(1,)`の形のTensor をとる`OrderedDict` 形式です。\n",
    "\n",
    "では、このデータセットの最初の二つの要素を `dataset.take(2)` を用いて取得し、numpy arrayを値に持つ通常の Python 辞書型に変換してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in tempds.take(2):\n",
    "    pprint({k: v.numpy() for k, v in data.items()})\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特徴量を変換する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで必要なのは、特徴量とラベルですので、この辞書に二つのことをする必要があります。\n",
    "\n",
    "1. 不必要な列を除外する。（ここでは存在しませんが、何をするべきかは定義しておきます）\n",
    "1. ラベルを特徴量から区別して保持する\n",
    "\n",
    "まず、一つのレコード（`tf.data.Dataset` に含まれる `OrderedDict` の形式）を入力に取り、以下の二つの要素を含むタプルを出力する関数を定義しましょう。\n",
    "\n",
    "- `OrderedDict` からラベルを除外したもの\n",
    "- ラベル (`fare_amount`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNWANTED_COLS = []\n",
    "\n",
    "\n",
    "def features_and_labels(row_data):\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "    features = row_data\n",
    "    \n",
    "    for unwanted_col in UNWANTED_COLS:\n",
    "        features.pop(unwanted_col)\n",
    "\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tempds` データセット内の二つのデータポイントに対して、 試しに`feature_and_labels` 関数を実行してテストしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row_data in tempds.take(2):\n",
    "    features, label = features_and_labels(row_data)\n",
    "    pprint(features)\n",
    "    print(label, \"\\n\")\n",
    "    \n",
    "    assert label.shape == [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ミニバッチ処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`create_dataset` 関数をリファクタリングし、`batch_size` を引数にとってデータをバッチに分ける機能を追加しましょう。\n",
    "また、実装した`features_and_labels`関数を使用して、特徴量とラベルのタプルを生成しましょう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(pattern, batch_size):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        pattern, batch_size, CSV_COLUMNS, DEFAULTS)\n",
    "    return dataset.map(features_and_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バッチが正しいサイズになっているかどうかテストしましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "tempds = create_dataset('../data/taxi-train*', batch_size=BATCH_SIZE)\n",
    "\n",
    "for X_batch, Y_batch in tempds.take(2):\n",
    "    pprint({k: v.numpy() for k, v in X_batch.items()})\n",
    "    print(Y_batch.numpy(), \"\\n\")\n",
    "    assert len(Y_batch) == BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### シャッフル\n",
    "\n",
    "ディープラーニングのモデルを複数のワーカーでミニバッチ学習をする際、データをシャッフルするのが有効です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`create_dataset` 関数をリファクタリングして、データのシャッフルが行えるようにしましょう。\n",
    "\n",
    "ここで、関数の内部で場合分けをすることができるように、新たに `mode` という引数を追加しましょう。<br>\n",
    "たとえば、トレーニング(`mode == tf.estimator.ModeKeys.TRAIN`)の際にはデータをシャッフルし、検証時(`mode == tf.estimator.ModeKeys.EVAL`)にはシャッフルは行わない場合には、以下のように書くことができます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(pattern, batch_size=1, mode=tf.estimator.ModeKeys.EVAL):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        pattern, batch_size, CSV_COLUMNS, DEFAULTS)\n",
    "\n",
    "    dataset = dataset.map(features_and_labels).cache()\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        dataset = dataset.shuffle(1000).repeat()\n",
    "\n",
    "    # take advantage of multi-threading; 1=AUTOTUNE\n",
    "    dataset = dataset.prefetch(1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この二つのモードがうまく動いているか確認しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempds = create_dataset('../data/taxi-train*', 2, tf.estimator.ModeKeys.TRAIN)\n",
    "print(list(tempds.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempds = create_dataset('../data/taxi-valid*', 2, tf.estimator.ModeKeys.EVAL)\n",
    "print(list(tempds.take(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで、データのパイプラインの作り方を学習することができました。<br>\n",
    "次のノートブックでは、この入力パイプラインを利用してモデルを構築します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google Inc.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m61"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
