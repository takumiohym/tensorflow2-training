{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Explore and create ML datasets </h1>\n",
    "\n",
    "このノートブックでは、New Yorkでのタクシー乗車料金を予測する機械学習モデルを作成するために、その前段階として使用するデータの探索を行います。<br>\n",
    "\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "では、必要なPythonライブラリのimportから始めていきましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> BigQueryからサンプルデータを取得する</h3>\n",
    "\n",
    "\n",
    "これから、<a href=\"https://console.cloud.google.com/bigquery?GK=nyc-tlc&page=table&t=trips&d=yellow&p=nyc-tlc\">BigQueryのpublic dataset</a>を利用します。リンクをクリックして、カラム名をチェックしましょう。<br>\n",
    "[Detail]タブに移動してレコード数が10億行以上あることを確認し、[Preview]タブでいくつかのデータを確認してみてください。\n",
    "\n",
    "\n",
    "では、SQLを書いていくつかのフィールドをピックアップしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "  SELECT\n",
    "    pickup_datetime, pickup_longitude, pickup_latitude, dropoff_longitude,\n",
    "    dropoff_latitude, passenger_count, trip_distance, tolls_amount, \n",
    "    fare_amount, total_amount \n",
    "  FROM `nyc-tlc.yellow.trips`\n",
    "  LIMIT 10\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()\n",
    "trips = client.query(sql).to_dataframe()\n",
    "trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "レコード数を増やしてグラフを書いてみましょう。返されるレコードの順番は保証されないため、LIMITの数を増やしてもどのレコードが返ってくるかはわかりません。<br>\n",
    "適切なデータサンプルを取得するために、乗車時刻のHASHを利用して1万行に10行だけを取得しましょう。つまり、10億行のデータがあれば、およそ1万行のデータが取得できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "  SELECT\n",
    "    pickup_datetime,\n",
    "    pickup_longitude, pickup_latitude, \n",
    "    dropoff_longitude, dropoff_latitude,\n",
    "    passenger_count,\n",
    "    trip_distance,\n",
    "    tolls_amount,\n",
    "    fare_amount,\n",
    "    tip_amount,\n",
    "    total_amount\n",
    "  FROM\n",
    "    `nyc-tlc.yellow.trips`\n",
    "  WHERE\n",
    "    MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), 100000) = 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = client.query(sql).to_dataframe()\n",
    "print('sample size: {}'.format(len(trips)))\n",
    "trips[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> データを探索する</h3>\n",
    "\n",
    "では、データを探索し、必要に応じてクリーンアップをしていきましょう。ここでは、PythonのSeabornパッケージを利用してグラフをビジュアライズし、Pandasを使ってスライシングとフィルタリングを行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.regplot(x=\"trip_distance\", y=\"fare_amount\", fit_reg=False, ci=None, truncate=True, data=trips)\n",
    "ax.figure.set_size_inches(10, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なにかおかしな点があることに気づきますか？\n",
    "\n",
    "どうやら、乗車距離が0であったり、乗車料金が明らかに外れ値であったりなどの無効なデータがたくさんあるようです。<br>\n",
    "これらは分析対象から除外しましょう。BigQueryへのクエリを変更して、乗車距離が0マイルより大きく、かつ乗車料金が最低乗車料金（$2.50）以上のデータに絞り込むことができます。\n",
    "\n",
    "追加されたWHERE句に注目してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "  SELECT\n",
    "    pickup_datetime,\n",
    "    pickup_longitude, pickup_latitude, \n",
    "    dropoff_longitude, dropoff_latitude,\n",
    "    passenger_count,\n",
    "    trip_distance,\n",
    "    tolls_amount,\n",
    "    fare_amount,\n",
    "    tip_amount,\n",
    "    total_amount\n",
    "  FROM\n",
    "    `nyc-tlc.yellow.trips`\n",
    "  WHERE\n",
    "    MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), 100000) = 1\n",
    "    AND trip_distance > 0 AND fare_amount >= 2.5\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = client.query(sql).to_dataframe()\n",
    "ax = sns.regplot(x=\"trip_distance\", y=\"fare_amount\", fit_reg=False, ci=None, truncate=True, data=trips)\n",
    "ax.figure.set_size_inches(10, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "45ドル、50ドル付近の直線は何でしょうか？これは、たとえばマンハッタンのJFK空港、La Guardia空港からの定額料金と推測できます。\n",
    "\n",
    "では、データを確認して、値が何を意味しているのかを確認してみましょう。toll_amount（通行料金）、乗車料金(fare_amount)とtotal_amount（総額料金）の関係に注目してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tollrides = trips[trips['tolls_amount'] > 0]\n",
    "tollrides[tollrides['pickup_datetime'] == pd.Timestamp('2010-04-29 12:28:00', tz = 'UTC')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上のいくつかのサンプルデータを見てみると、総額料金(total_amount)は乗車料金(fare_amount)と通行料金(tolls_amount）を反映していることが分かります。そして任意のチップ(tip_amount)が加えられていますが、チップを現金で支払った場合にはチップの値段はわかりません。\n",
    "そのため、ここではfare_amount + tolls_amountを予測の対象として利用します。チップは乗客の裁量によるため、料金予測ツールからは除外するべきでしょう。\n",
    "\n",
    "では、カラムごとの値の分布を確認しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "どうやら経度(longitude)と緯度(latitude)の最小値と最大値に問題があるようです。\n",
    "\n",
    "いくつかの履歴の乗車(pickup)位置と降車(dropoff)位置を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showrides(df, numlines):\n",
    "  lats = []\n",
    "  lons = []\n",
    "  for iter, row in df[:numlines].iterrows():\n",
    "    lons.append(row['pickup_longitude'])\n",
    "    lons.append(row['dropoff_longitude'])\n",
    "    lons.append(None)\n",
    "    lats.append(row['pickup_latitude'])\n",
    "    lats.append(row['dropoff_latitude'])\n",
    "    lats.append(None)\n",
    "\n",
    "  sns.set_style(\"darkgrid\")\n",
    "  plt.figure(figsize=(10,8))\n",
    "  plt.plot(lons, lats)\n",
    "\n",
    "showrides(trips, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showrides(tollrides, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "予想どおり、通行料金(toll_amount)を含む乗車は他の一般的な乗車よりも距離が長いようです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> データクレンジングとその他の前処理</h3>\n",
    "\n",
    "以下のデータクレンジングが必要です。\n",
    "\n",
    "<ol>\n",
    "<li>New Yorkの経度は-74前後、緯度は41前後のため、それ以外の乗降は除外する</li>\n",
    "<li>乗客が0人のデータは除外する</li>\n",
    "<li>総額料金(total_amount)を、乗車料金(fare_amount)と通行料金(tolls_amount)だけを反映するように変更する。また使用した2つの列は除外する</li>\n",
    "<li>乗車時には、乗車位置と降車位置は分かっているが、乗車距離はまだ分からない（乗車距離はルートによって変わるため）。そのため乗車距離は機械学習のデータセットには含めない</li>\n",
    "<li>タイムスタンプを削除する</li>\n",
    "</ol>\n",
    "\n",
    "距離0の乗車を除外したのと同様にBigQueryを使って前処理を行うことができますが、ここではPythonとPandasを使いましょう。<br>\n",
    "本番環境では、リアルタイムの入力データに対して同様の前処理を行う必要があります。\n",
    "\n",
    "入力データに対するこのような前処理は、機械学習を行う際に一般的です。\n",
    "\n",
    "では、トレーニングに使用するデータを作成するために、WHERE句で絞り込む値を変更し、20万件程のデータを取得してクレンジングを適用しましょう<br>\n",
    "（クエリには１分ほどの時間がかかります）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "  SELECT\n",
    "    pickup_datetime,\n",
    "    pickup_longitude, pickup_latitude, \n",
    "    dropoff_longitude, dropoff_latitude,\n",
    "    passenger_count,\n",
    "    trip_distance,\n",
    "    tolls_amount,\n",
    "    fare_amount,\n",
    "    tip_amount,\n",
    "    total_amount\n",
    "  FROM\n",
    "    `nyc-tlc.yellow.trips`\n",
    "  WHERE\n",
    "    MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), 5000) = 1\n",
    "    AND trip_distance > 0 AND fare_amount >= 2.5\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = client.query(sql).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dayofweek(t):\n",
    "    return t.dayofweek\n",
    "\n",
    "def extract_hour(t):\n",
    "    return t.hour\n",
    "\n",
    "def preprocess(trips_in):\n",
    "    trips = trips_in.copy(deep=True)\n",
    "    trips.fare_amount = trips.fare_amount + trips.tolls_amount\n",
    "    trips['dayofweek'] = trips['pickup_datetime'].apply(extract_dayofweek)\n",
    "    trips['hourofday'] = trips['pickup_datetime'].apply(extract_hour)\n",
    "    del trips['tolls_amount']\n",
    "    del trips['total_amount']\n",
    "    del trips['tip_amount']\n",
    "    del trips['trip_distance']\n",
    "    del trips['pickup_datetime']\n",
    "    qc = np.all([\\\n",
    "             trips['pickup_longitude'] > -78, \\\n",
    "             trips['pickup_longitude'] < -70, \\\n",
    "             trips['dropoff_longitude'] > -78, \\\n",
    "             trips['dropoff_longitude'] < -70, \\\n",
    "             trips['pickup_latitude'] > 37, \\\n",
    "             trips['pickup_latitude'] < 45, \\\n",
    "             trips['dropoff_latitude'] > 37, \\\n",
    "             trips['dropoff_latitude'] < 45, \\\n",
    "             trips['passenger_count'] > 0,\n",
    "            ], axis=0)\n",
    "    del trips['passenger_count']\n",
    "    return trips[qc]\n",
    "\n",
    "tripsqc = preprocess(trips)\n",
    "tripsqc.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データクレンジングにより、およそ5000行(222460 - 217514)、全体の2％ほどのデータが除外されました。これは良いバランスです。\n",
    "\n",
    "では、機械学習用データセットを作成しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 機械学習用データセットの作成 </h3>\n",
    "\n",
    "クレンジングされたデータを学習用と検証用、テスト用のデータセットにランダムに分割しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled = tripsqc.sample(frac=1)\n",
    "trainsize = int(len(shuffled['fare_amount']) * 0.70)\n",
    "validsize = int(len(shuffled['fare_amount']) * 0.15)\n",
    "\n",
    "df_train = shuffled.iloc[:trainsize, :]\n",
    "df_valid = shuffled.iloc[trainsize:(trainsize+validsize), :]\n",
    "df_test = shuffled.iloc[(trainsize+validsize):, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3つのDataframeをcsvファイルに書き出しましょう。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(df, filename):\n",
    "    outdf = df.copy(deep=False)\n",
    "    # reorder columns so that target is first column\n",
    "    cols = ['fare_amount',\n",
    "            'dayofweek',\n",
    "            'hourofday',\n",
    "            'pickup_longitude',\n",
    "            'pickup_latitude',\n",
    "            'dropoff_longitude',\n",
    "            'dropoff_latitude',\n",
    "           ]\n",
    "    outdf = outdf[cols]\n",
    "    outdf.to_csv(filename, index_label=False, index=False)\n",
    "\n",
    "path = '../data'\n",
    "to_csv(df_train, '{}/taxi-train.csv'.format(path))\n",
    "to_csv(df_valid, '{}/taxi-valid.csv'.format(path))\n",
    "to_csv(df_test, '{}/taxi-test.csv'.format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> データセットが存在することを確認する </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l ../data/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習、検証、テストに対応する３つのcsvファイルが作成されました。ファイルサイズの割合はデータの分割の割合に対応しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head ../data/taxi-train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上手くいっていますね！機械学習用のデータセットが作成され、機械学習モデルのトレーニング、検証、評価の準備ができました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
